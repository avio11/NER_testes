{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import joblib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('data/data_aposentadoria/labeled.csv')\n",
    "df = pd.read_csv('data/labeled.csv')\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 168, 168, 73, 73)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_features(sentence):\n",
    "  sentence_features = []\n",
    "  for j in range(len(sentence)):\n",
    "    word_feat = {\n",
    "            'word': sentence[j].lower(),\n",
    "            'capital_letter': sentence[j][0].isupper(),\n",
    "            'all_capital': sentence[j].isupper(),\n",
    "            'isdigit': sentence[j].isdigit(),\n",
    "            'word_before': sentence[j].lower() if j==0 else sentence[j-1].lower(),\n",
    "            'word_after:': sentence[j].lower() if j+1>=len(sentence) else sentence[j+1].lower(),\n",
    "            'BOS': j==0,\n",
    "            'EOS': j==len(sentence)-1\n",
    "    }\n",
    "    sentence_features.append(word_feat)\n",
    "  return sentence_features\n",
    "\n",
    "def separate_cols(arq):\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(arq)):\n",
    "        x.append(extract_features(arq['text'][i].split()))\n",
    "        y.append(arq['labels'][i].split())\n",
    "    return x, y\n",
    "\n",
    "x, y = separate_cols(df)\n",
    "train_x = x[:math.floor(0.7*len(df))]\n",
    "train_y = y[:math.floor(0.7*len(df))]\n",
    "test_x = x[math.floor(0.7*len(df)):]\n",
    "test_y = y[math.floor(0.7*len(df)):]\n",
    "\n",
    "len(df), len(train_x), len(train_y), len(test_x), len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn_crfsuite.CRF(\n",
    "    algorithm = 'l2sgd', \n",
    "    c2=1,\n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "model.fit(train_x, train_y)\n",
    "y_pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665589021850224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Instalations\\envs\\ner\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1464: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(\n"
     ]
    }
   ],
   "source": [
    "labels = list(model.classes_)\n",
    "labels.remove('O')\n",
    "\n",
    "f1 = metrics.flat_f1_score(test_y, y_pred, \n",
    "                      average='weighted', labels=labels)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         precision    recall  f1-score   support\n",
      "\n",
      "                 B-nome      1.000     0.963     0.981        27\n",
      "                 I-nome      1.000     0.963     0.981        80\n",
      "                B-cargo      0.963     1.000     0.981        26\n",
      "                I-cargo      0.976     1.000     0.988       166\n",
      "               B-classe      1.000     0.913     0.955        23\n",
      "               I-classe      1.000     0.913     0.955        23\n",
      "     B-fundamento_legal      0.986     0.973     0.979        73\n",
      "     I-fundamento_legal      0.962     0.975     0.968       709\n",
      "            B-matricula      1.000     0.963     0.981        27\n",
      "            I-matricula      1.000     0.974     0.987        38\n",
      "               B-padrao      1.000     0.957     0.978        23\n",
      "               I-padrao      1.000     0.957     0.978        23\n",
      "               B-quadro      1.000     1.000     1.000        23\n",
      "               I-quadro      1.000     1.000     1.000       235\n",
      "             B-processo      0.955     0.955     0.955        22\n",
      "             I-processo      0.943     0.943     0.943        35\n",
      "             B-vigencia      0.000     0.000     0.000         0\n",
      "             I-vigencia      0.000     0.000     0.000         0\n",
      "B-tipo_de_aposentadoria      0.000     0.000     0.000         4\n",
      "I-tipo_de_aposentadoria      0.000     0.000     0.000        13\n",
      "      B-matricula_siape      0.000     0.000     0.000         0\n",
      "      I-matricula_siape      0.000     0.000     0.000         0\n",
      "\n",
      "              micro avg      0.977     0.967     0.972      1570\n",
      "              macro avg      0.718     0.702     0.709      1570\n",
      "           weighted avg      0.967     0.967     0.967      1570\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Instalations\\envs\\ner\\lib\\site-packages\\sklearn\\utils\\validation.py:68: FutureWarning: Pass labels=['B-nome', 'I-nome', 'B-cargo', 'I-cargo', 'B-classe', 'I-classe', 'B-fundamento_legal', 'I-fundamento_legal', 'B-matricula', 'I-matricula', 'B-padrao', 'I-padrao', 'B-quadro', 'I-quadro', 'B-processo', 'I-processo', 'B-vigencia', 'I-vigencia', 'B-tipo_de_aposentadoria', 'I-tipo_de_aposentadoria', 'B-matricula_siape', 'I-matricula_siape'] as keyword args. From version 0.25 passing these as positional arguments will result in an error\n",
      "  warnings.warn(\"Pass {} as keyword args. From version 0.25 \"\n",
      "E:\\Instalations\\envs\\ner\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "E:\\Instalations\\envs\\ner\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(metrics.flat_classification_report(\n",
    "    test_y, y_pred, labels=labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aposentadoria_ner.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save trained model\n",
    "joblib.dump(model, \"aposentadoria_ner.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
